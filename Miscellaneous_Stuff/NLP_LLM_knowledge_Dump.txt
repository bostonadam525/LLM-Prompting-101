This is a general "knowledge dump" of what I know about LLMs and NLP
By Adam Lang
Date: 6/3/2024
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Generative AI Models I have worked with
================================================================================================================================================================================
- GPT 3.5 turbo
- GPT 4.0
- llama 2, llama 3
- BERT
- Flan T5 (encoder-decoder)
- Whisper
- Wav2Vec

Medical Specific Models
- RadBERT
- BiomedBERT (pubmedBERT)
- Clinical-AI-Apollo-medical-NER
- Falconsai-T5 Large for Medical Text Summarization

--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
LLMs Overview - Focus on Amazon Bedrock
================================================================================================================================================================================
Foundation Models (broad categorization of Models)
- large deep learning neural networks + trained on massive datasets
- Generate text, images, videos or both 


Focus area: Text Generation 
- LLMs
- example: GPT4, llama 3 


Focus area: Image Generation 
- image generation or Diffusion model
- image generation: text to image or image to image 
- example: Stable Diffusion, DALLE-3

Focus area: Text + Vision
- 'Multi modal' models 
- Example: Claude3 (Anthropic)
- Healthcare interesting use case: image x-ray + text input --> output 



Gen AI from AWS 
1. Amazon EC2 Trn1n + EC2 Inf2
	* IaaS - infrastructure as a service (self managed) for ML experts to build, train and deploy your own models.
	* AWS manages physical server --> everything else the customer 
	* AWS Inferentia -- lowest cost per inference in the cloud for running DL models 
	* AWS Inferentia2 -- high performance at lowest cost per inference for LLMs and diffusion models 
	* AWS Tranium -- most cost efficent for LLM and diffusion models => training models 
2. Amazon Bedrock
	* fully managed serverless service offers choice of high-performing foundation models from leading AI companies. 
	* Serverless --> pay only what you use! 
3. Amazon Sagemaker Foundation Model Hub (JumpStart)
	* provides pretrained, open-source models 
	* deploy open source foundation models with custom configurations
	* Instance type depends on what you pay 
4. Amazon CodeWhisperer 
	* AI coding companion -- 15 programming languages for code generation 
--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Prompting Techniques
================================================================================================================================================================================
* General Concepts
- vary and iterate prompts
- use delimeters
- specify structured outputs
- use temperature parameter

Prompt Techniques (Specific)
- few shot
- one shot 
- zero shot 
- multi-step
- iterative 
- step back (Sometimes search quality and model generations can be tripped up by the specifics of a question. One way to handle this is to first generate a more abstract, "step back" question and to query based on both the original and step back question.)
- etc...

-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Retrieval Augmented Generation (RAG)
* I have a specific repo devoted to this but will list some of the techniques here that I have used or experimented with:
===============================================================================================================================================================================

1. Chunking
	- fixed-size chunking
	- content-aware chunking 
	- recursive chunking 
	- specialized chunking
	- medical text chunking 
	
2. Vectorization and Embeddings
* MTEB leaderboard
* Word embeddings vs. Sentence Embeddings 


3. Search Index 
- Vector store index 
	* Flat index 
	* Vector index 
	* Nearest neighbors (clustering, trees, HNSW)

- Hierarchical Indices
	* Summaries
	* Document Chunks (retrieve ONLY relevant docs --> then search top K)

- Context Enrichment --> retrieve smaller chunks for better quality + add up or include surrounding text or sentences for semantic context 
	* Sentence Window Retrieval ("sliding window")
	* Auto-merging retrieval (similar to sentence window --> top k split --> hierarchical linkage)
	

- Fusion Retrieval (hybrid search)
	* keyword (lexical) search --> algorithm such as TF-IDF or BM25 (most common)
	* semantic (vector) search 
	
	* Combine the 2 methods using "Reciprocal rank fusion" algorithm 
		* Commonly done with "cosine similarity score"
		
4. Reranking 
- standard RAG uses embeddings pre-computed with bi-encoder --> compresses document into fixed length vectors --> vector DB 
- bi-encoders are fast and easy to scale --> but biencoders do not know anything about semantic relationships between sentences 
- cross-encoders --> query + retrieved document encoded simultaneously



RAG Metrics 
- Hit Rate 
- Mean Reciprocal Rank (MRR)
- Mean Average Precision (MAP)
- Normalized Discounted Cumulative Gain (NDCG)



5. Rerank + Hybrid search 
- hybrid retrieval + semantic ranking --> grounds LLM applications 


6. Contextual Compression 
- minimizes storage footprint of data and preserving information 
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Vector Databases
===============================================================================================================================================================================
* Vector DBs that I have used:
1. MongoDB Atlas
2. Chroma
3. Pinecone
4. Weaviate

* Looking to experiment with Graph DBs as well (i.e. Neo4j)
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Structured Outputs from LLMs
===============================================================================================================================================================================
- Prompting vs. fine tuning hyperparameters vs. serialization (i.e. pydantic)
- Fine tuning Methods + process

supervised fine tuning 
- basic hyperparameters
- transfer learning 
- domain specific 
- multi-task learning 
- few shot learning 
- task-specific fine-tuning


RLHF
- reward modeling 
- proximal policy optimization (PPO)
- comparative ranking 
- preference learning (reinforcement learning with preference feedback)
- parameter efficient fine tuning (PEFT)
	* LoRA
	* Prefix tuning
	* P tuning
	* Prompt Tuning
	* QLoRA


Fine tuning process 
1. Data preparation 
2. Choose a pre-trained model for right task 
3. Choose correct parameters for fine-tuning 
4. Validation 
5. Model Iteration
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
LLM Evaluation Metrics
===============================================================================================================================================================================
LLM benchmarks
- GLUE
- SuperGLUE
- HellaSwag
- TruthfulQA
- MMLU

Eval frameworks/platforms
- Azure AI studio evaluation (MSFT)
- Prompt Flow (MSFT)
- Weights & Biases
- LangSmith (from LangChain)
- TruLens (TruEra)
- VertexAI (Google)
- AWS Bedrock
- DeepEval (Confident AI)
- Parea AI 

Good overview article: https://medium.com/data-science-at-microsoft/evaluating-llm-systems-metrics-challenges-and-best-practices-664ac25be7e5


Evaluation Techniques
- Human evaluation ('eyeballing')
- Human Annotation (aka supervised evaluation)
- Standard machine learning metrics 
	* Exact match 
	* F1 score
	* Precision
	* Recall
	* Accuracy
	* Confusion Matrix
	
- Code eval datasets 
- holistic eval of language models 
- robustness
- inference efficiency 
- case specific benchmarks 
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Classical NLP Techniques
- These are still important when building, fine-tuning or working with LLMs
===============================================================================================================================================================================
1. Text classification
2. Named Entity Recognition (NER)
3. Named Entity Linking
4. Encoding text 
  * one-hot (present vs not)
  * CountVectorizer (most frequent words)
  * TF-IDF (least frequent words in corpus)
5. Python Frameworks
  * spacy
  * NLTK
  * Gensim
6. N-grams
7. Text pre-processing
  a. Tokenization
    * BPE
    * sub-word
    * character level
    * white space
    * Autotokenizer (hugggingface)
    * others....
  b. Stemming
  c. Lemmatization
  d. Morphemes
  e. Phonemes (CMU dict)

8. Vectorization + Embeddings
* Pre-built embeddings vs. custom?
* MTEB leaderboard
* word vs. sentence embeddings (SBERT)

9. Text splitting
  * recursive text splitting most common, others...
- HTML header
- HTML section 
- character
- code 
- markdown header 
- recursive JSON 
- semantic chunking/splitter
- tokens 
- etc...
-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Deep Learning
===============================================================================================================================================================================
- Besides "classical NLP" there are "classical" Deep Learning Algorithms that are still used in addition to the modern day Transformer.

1. ANN
- multi-layer perceptron (MLP) is a basic feedforward NN
- if there is more than 1 hidden layer => Deep ANN


2. RNN
- sequential data (text, audio, video, etc..)
- previous input ==> produces output 
- weights and biases for all nodes in the layer are the SAME.

problem: SHORT-TERM MEMORY due to vanishing gradients
- during backpropagation --> update model weights --> keep losing information! 

solution: GRUs and LSTMs

3. GRU (gated recurrent units)
- GRUs have 2 gates (reset gate, update gate)
- each gate has its own NN

4. LSTM (long short term memory)
- similar to GRUs
- 3 gates:
	1. forget gate 
	2. input gate 
	3. output gate 
- all gates use sigmoid function => output gate values between 0 and 1 


5. CNN
- convolutional layers
	* sliding windows/grids with filters/kernels => matrix multiplication
	* step
- rectified linear units (ReLU)
	* activation function applied after each convolutional operation 
	* logits can be processed for non-linear relationships in the data 
	* helps with vanishing gradients
- pooling layers 
	* extracts the most important/significant features from convolutions 
	* aggregate operations (max, sum, avg)
	* helps mitigate overfitting
- fully connected layers
	* last layer of convolutional neural network 
	* inputs correspond to flattened 1D matrix generated by last pooling layer 
	
- Softmax prediction layer generates probabilities 

Mitigation for overfitting 
- Dropout
- Batch normalization
- Pooling layers 
- Early stopping
- Noise injection 
- L1 and L2 normalization 
- Data augmentation 


6. Transformers
  Transformers 
- excel at sequential data (attention is all you need)
- transformers are "parallelizable" unlike RNNs
- transformers can capture long range dependencies (unlike RNNs or CNNs)\
- no assumptions about temporal/spatial relationships across objects 


input embeddings
positional encodings 
transformer block
	* multiple blocks stacked together 
	* Each block has:
		1. multi-head self-attention mechanism (weight token importance in a sequence)
		2. position-wise feed-forward neural network
		
linear blocks 
- linear block (fully connected layer/dense layer) --> linear mapping from vector space to original input domain
	* important for "decision making" in the prediction

softmax block 
- final stage that takes logit scores -> normalizes into probability distribution
- each softmax output is the model's confidence prediction for a class or token. 


Types of transformers 
1. Bidirectional (BERT) - ENCODER ONLY
- words in relation to all other words in a sentence NOT in isolation
- uses bidirectional masked language model (MLM)
- training -> random masks --> predict these tokens based on context
- main importance: left to right and right to left modeling

When to Use an encoder only model? 
- Ideal when you want to convert variable-length input sequences into fixed-length representations or embeddings.
- Encoders are typically employed to process the input sequence, such as a sentence or document, and capture its semantic meaning or contextual information.
- The encoder can be based on recurrent neural networks (RNNs), such as LSTM or GRU, or transformer-based architectures like the Transformer encoder.
- Common use cases for encoders include:
	a. text classification
	b. sentiment analysis
	c. machine translation
	d. text generation from prompts

2. Generative pretrained transformers (GPT) - DECODER ONLY
- stacked transformer decoders 
- Autoregressive --> regress/predict next word or sequence based on previous value/word/sequences.

When to use a decoder only model?
- Use a decoder when you want to generate output sequences based on the fixed-length representations obtained from the encoder.
- Decoders are employed in tasks where the output sequence is dependent on the input sequence, such as language translation or text generation.
- The decoder takes the fixed-length representation and generates a sequence, one token at a time, autoregressively.
- The decoder can be based on RNNs, such as LSTM or GRU, or transformer-based architectures like the Transformer decoder or the GPT (Generative Pre-trained Transformer) model.
- Decoders are commonly used in:
	a. machine translation, 
	b. text summarization
	c. image captioning
	d. dialogue generation.


3. Bidirectional and autoregressive transformers (BART, T5) - ENCODER + DECODER
- combines both BERT bidirectional encoder + GPT autoregressive decoder
- generates output sequence 1 token at at time 

Why use ENCODER-DECODER models?
- Typically used for natural language processing tasks that involve:
	a. understanding input sequences and generating output sequences, often with different lengths and structures. 
	a. They are very good at tasks where there is a complex mapping between the input and output sequences and where it is crucial to capture the relationships between the elements in both sequences.

What are some common use cases for encoder-decoder models?
	a. text translation
	b. text summarization.

4. Multimodal transformers 
- many inputs: usually image + text 

5. Vision transformers 
- unlike CNNs before them, these models process an image as a sequence of fixed-size patches (similar to words in a sentence).
- Global self attention helps these models. 


AUTOENCODERS
- Autoencoder is a unsupervised deep learning network. 
- It learns efficient data representation, which means encoding. 
- It is unsupervised deep learning, more specifically it is self supervised deep learning as it is directed type of Neural Network which does not too much dependent on training data, but it can generate own labeled data.

DIFFUSION MODELS
- Diffusion models can be considered a type of autoencoder, but they have different purposes and aren't interchangeable:
- Diffusion models are built by training a neural network to invert a procedure that gradually turns data into noise. The underlying idea is to learn how to denoise examples of a distribution. Diffusion models can be used to generate high-quality images that are close to what a camera would take. However, their image outputs can sometimes be noisy and blurry.

- Autoencoders: These neural networks are trained to predict their input.

- Diffusion models can also be formulated as masked autoencoders (DiffMAE) by conditioning them on masked input. This approach has several capabilities, including image inpainting, classification accuracy, and initialization for downstream recognition tasks.
-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Machine Learning - Bias-Variance Trade off and Overfitting vs. Underfitting
=============================================================================================================================================================================================================================
Bias
* Inability for a machine learning model to capture the true relationship in the data. 

Metrics
1. mean squared error (MSE)
2. mean absolute error (MAE)
sums of squares --> how well outcome fits data



Variance
- difference in fits between data sets 




low bias 
-- model can easily adapt to new data 

high variability
-- vastly different sums of squares for different data sets 



high bias 
-- example would be straight line (bias to this prediction)

low variability
-- sums of squares does not vary, little error from the prediction always 



Overfitting
- training data is fit too well
- model is too complex 
-- low training error
-- high test error 

Underfitting
- training data is fit poorly
- model is not complex enough 
-- high training error 
-- high test error 


Machine Learning ideal algorithms
-- low bias 
-- low variability (consistent predictions across different datasets)



How do we find "sweet spot" between simple and complicated models?
- regularization
- boosting
- bagging 



-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
Python
1. PyTorch
2. TensorFlow

Huggingface transformers library 
